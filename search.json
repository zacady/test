[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for the Social Sciences",
    "section": "",
    "text": "Preface\nComing soon!\nIn the meantime, you can skip to the intro :)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction: turning questions into models",
    "section": "",
    "text": "1.1 Variables\nWe have seen that well-being can usefully be considered as a variable to convey that it is not identical for everyone and at all times. The same is true for things like personality, gender, income, or nationality. You might notice that, by calling all these information “variables”, we are lumping quite different things together: income, for instance, is a quantity - it can meaningfully be represented by a number - while gender is not. Distinctions of this sort are important, and we will discuss them in due course. For now, suffice it to say that",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: turning questions into models</span>"
    ]
  },
  {
    "objectID": "intro.html#variables",
    "href": "intro.html#variables",
    "title": "1  Introduction: turning questions into models",
    "section": "",
    "text": "A variable is a quantity or a quality that varies.\n\n\n1.1.1 Theoretical and operational variables\nBut what do we mean exactly when we use the word “well-being”? Is it the same as happiness? Or satisfaction with one’s life? One of your tasks as a researcher will be to define your phenomenon of interest as clearly possible. We call such clearly defined concept a theoretical variable (or a theoretical construct).\nFor instance, having thoroughly reviewed the existing knowledge on the topic, you might conclude that the notion of well-being can be defined as a special case of the sentiment of satisfaction. This sentiment of satisfaction, you add, is not related to any particular aspect of the person’s life, but rather about all aspects considered together. Hence, you decide to define well-being as an individual’s degree of satisfaction with their life considered in all its aspects. This would be your theoretical variable.\nNote that defining a theoretical variable (well-being, in this case) has less do to with laying out what well-being ‘truly is’ than with being clear about what you mean when using the term.1 After all, definitions of terms are conventional: a term can be said to have a “true meaning” only in the sense of a meaning that is agreed upon by some set of people. What matters the most when defining your theoretical variable is to be clear – i.e., to leave as little ambiguity as possible regarding what it does and does not refer to in the real world. But this does not mean that you can just ignore how you theoretical variable is usually defined by other scientists, or even among the general public. Conventions in terminology facilitate smooth communication, and departing from them can result in misunderstandings. Claiming to study “well-being” while actually defining that term as an individual’s preference for cats over dogs would be confusing for no reason, no matter how clear the definition. In sum, you should depart from conventional definitions only if you have good reasons to do so, such as when it helps distinguishing your theoretical variable better from closely related concepts.\n\nA theoretical variable is the concept of interest defined in abstract and general terms.\n\nHaving clearly defined your theoretical variable is not enough, however, because such a variable cannot be directly observed the real world. Think about well-being: however clearly you define it, you cannot simply observe a person and self-evidently know their well-being. You will need to find a concrete procedure to extract the information corresponding to your theoretical concept in the real world. Such procedure is what we call measurement. And the information obtained through measurement is the operational variable.\nSince in our example we defined our theoretical variable in subjective terms – through the feeling of satisfaction – you could reason that the best way to extract the corresponding information would be to directly ask people. You might thus devise a questionnaire including a question similar to the following: thinking of your life as whole, how satisfied would you say you are on a scale ranging from 1 (not satisfied at all) to 10 (completely satisfied)?\nIdeally, you should select a measurement procedure with the aim that the resulting operational variable should faithfully and reliably reflect the theoretical variable. Consider the measure of well-being proposed in the previous paragraph, for instance. If an individual happen to be in a bad mood while answering the question, the negative aspects of her life might come to her attention more readily than the positive ones. This might lead her to rate her own well-being lower than she would have earlier or later that day. If the aim is to capture satisfaction with one’s life as a whole, a measurement wouldn’t be very reliable if its outcome were to change with every mood swing. An alternative strategy could be to present participants with a series of questions, each focusing on a specific aspect of their life (e.g., finances, physical health, friendships, etc.) and aggregate all answers into a composite score. This way, participants would be prompted to pay attention to the different facets of their life instead of being guided by what their transient state of mind brings to their attention.\n\nAn operational variable is the information, obtained through a concrete measurement procedure, used to represent real-world realizations of the theoretical variable.\n\nThis example illustrates that devising a measurement procedure that yields a sound operational variable can be a tricky endeavour. Researchers have developed a set of tools that can help us in this task. We will discuss some of these tools later. In the meantime, examples of work attempting to defining well-being as a theoretical variable and proposing a rigorous way to measure it can be found in Diener et al (2010) and Weziak-Bialowolska et al (2021).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: turning questions into models</span>"
    ]
  },
  {
    "objectID": "intro.html#hypotheses",
    "href": "intro.html#hypotheses",
    "title": "1  Introduction: turning questions into models",
    "section": "1.2 Hypotheses",
    "text": "1.2 Hypotheses\nRemember what thinking in terms of variables taught us: to explain well-being, you would need to find explanations that account for its variability in the real world. And we saw that we could think of many possible explanations. Let us consider one of them: well-being is affected by income. We will call this statement a hypothesis. There are at least three features that a good hypothesis should satisfy.\nFirstly, one thing that makes the sentence well-being is affected by income a valid hypothesis is that it is a statement. On the contrary, the sentence is well-being affected by income? is a question rather than a statement and, hence, doesn’t qualify as a hypothesis.\nA second thing to note about this statement is that it indicates that well-being is explained by another variable: income. Just like well-being, income is not the same for everyone, and can vary from time to time for a given individual. Hence, income is also a variable. And our hypothesis states that this variable explains well-being. This is the form most hypotheses take: they are statements about a relationship between variables.\nThirdly, notice that we are stating something that we don’t actually know. Maybe income does affect well-being, maybe it doesn’t. We might have a strong belief about it, but it is crucial to always consider that our belief might turn out to be false. Indeed, the very purpose of a hypothesis is to formulate a factual statement that can be confronted with observations in the real world. Therefore, a statement qualifies as a hypothesis only if it is in principle possible to prove it wrong through observation. That is, we should be able to think of a situation in which what we observe will show us that the hypothesis is false, if it is indeed false. We say that a hypothesis must be falsifiable.\nTo make this last point clearer, consider the statement well-being is undermined by memories of childhood trauma even if those memories have been suppressed. Try to think of an observation that would prove this statement wrong. For instance, you could ask a large number of individuals to recall any childhood trauma and then compare the well-being of those who recall traumatic experiences to the well-being of those who don’t. But even if well-being turns to be exactly the same in both groups, the statement implies that those who don’t recall any trauma could have in fact suppressed such memories. Whatever observation we make, it seems that we can always “save” the statement from being proven wrong by invoking the notion that memories can be suppressed and, hence, can remain undetected. This statement thus appears unfalsifiable. It is not a valid hypothesis.\n\nA hypothesis is a statement, either about a variable or (more typically) about the relationship between variables, which is in principle falsifiable through observation.\n\nNote how the definition above leaves room for hypotheses that concern only one variable. For instance, the statement “Among humans, it is more frequent to be born a female than a male” is a valid hypothesis (it is a falsifiable statement) that refers to one variable only (sex).\n\n1.2.1 Dependent and independent variables\nWhen we hypothesize that two variables are related, we usually specify the relationship a bit further. If I say Well-being is affected by income, well-being and income are both variables, but one (well-being) is the thing we are trying to explain whereas the other (income) is the proposed explanation. We call them dependent variable and independent variable, respectively (see Figure 1.1).\n\nIn the context of a hypothesis, the dependent variable is the variable that we are trying to explain whereas the independent variable is the variable that we hypothesize to explain the dependent variable.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nIV\n\nIndependent\nvariable\n\n\n\nDV\n\nDependent\nvariable\n\n\n\nIV-&gt;DV\n\n\n\n\n\n\n\n\nFigure 1.1: Typical structure of a hypothesis\n\n\n\n\n\nNote that this not the only terminology used by scientists. A dependent variable is also called an outcome or a response variable. And an independent variable is also referred to as a predictor or an explanatory variable.\nOne reason often invoked for preferring these alternatives is that the terminology of dependent and independent variable is usually understood as implying a causal relationship. And causality is a very difficult thing to test in practice. Thus, when researchers consider that they are in no position to test the existence a causal relationship, they often opt for a terminology that does not evoke causality in the first place.\n\n\n1.2.2 Theoretical and operational hypotheses\nRecall our distinction that we made Section 1.1.1 between theoretical and operational variables. Since hypotheses talk about variables, it should seem natural to similarly distinguish two types of hypotheses:\n\nA theoretical hypothesis is a general statement about the relationship between theoretical variables.\nAn operational hypothesis is a statement about the relationship between operational variables in a clearly specified study setting.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: turning questions into models</span>"
    ]
  },
  {
    "objectID": "intro.html#modelling",
    "href": "intro.html#modelling",
    "title": "1  Introduction: turning questions into models",
    "section": "1.3 Modelling",
    "text": "1.3 Modelling\nAs social scientists, we should always remember that the phenomena we wish to explain are extremely complex. This means that we should abandon the illusion that we could explain them perfectly. Rather, we will rely on simplified descriptions which capture relevant and useful aspects of reality. We call such a simplification of reality a model.\n\n1.3.1 Why do we need models?\nTo understand why, let us consider once again hypotheses that could explain why well-being is not identical for everyone and at all times. It might be because well-being changes depending on variables like income, personality, physical and mental health, gender, life events, relationships, diet, sleep quality, and so on. The important thing to realize is that this list could go on. And on. Identifying all the variables that could potentially explain well-being is so difficult that we might as well consider it impossible in practice.\nBut even if we could know all the variables that affect well-being (which we can’t), grasping how exactly these variables influence well-being is way beyond what our minds can handle. Take the hypothesis that income fosters well-being: what amount of money exactly makes a difference? And does more income always lead to more well-being or is there an amount beyond which earning more doesn’t make any difference? Are there personalities or cultures for which income matters more for well-being? Such questions also could go on.\nAnd finally, even if we knew all the relevant variables and understood how exactly they shape well-being (which, again, we can’t), it would be impossible to measure them all and to do so with perfect accuracy. Measuring things takes time and resources, and measuring all the variables that might be relevant would require a gigantic amount of both. Moreover, some variables are very difficult to measure accurately. Think of any variable that is inherently subjective: well-being, personality, beliefs, emotions, and so on.\n\n\nThree reasons for humility\n\n\nIt is virtually impossible to identify all the variables that explain a given phenomenon.\nIt is virtually impossible to grasp how these variables influence our phenomenon of interest.\nIt is virtually impossible to measure all the variables that explain a given phenomenon and to do so with perfect accuracy.\n\n\n\n\n1.3.2 What is a model?\nThus, our attempts at explaining a phenomenon will necessarily result in a simplification of reality. This might sound disappointing. But simplifying a complex reality can actually be a very useful thing to do. Consider for instance a simple map of the world, like the one shown on the right of Figure 1.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: Example of model (the world map) of a complex reality (the earth’s surface)\n\n\n\n\nIn a sense, we can say that such a map describes the surface of our planet. Of course, it does so in a very incomplete and inaccurate way: lots of complicated stuff that characterize the earth’s surface at any given time are not depicted on this map, like weather-related processes happening in the atmosphere, variations in topography, the organisation and movement of tectonic plates, water streams circulating in the oceans or the many billions of living creatures populating the surface. But it is precisely because such a map is an incomplete description that it is useful to us. By extracting a few features from a highly complex reality, it makes it manageable for our minds to grasp relevant information which, otherwise, would be drown in an ocean of complexity. Thanks to the simplicity of the map, we can clearly identify the parts of the earth’s surface that are not covered by water, how this landmass was partitioned into countries by humans, and their geographic locations relative to the poles and the equator. The map is thus a simplification of the reality it aims to describe – i.e., the earth’s surface. But it is an informative simplification. We can say that the map is a model of the earth’s surface.\n\nA model is a simplified representation, which aims at describing relevant aspects of reality while ignoring others.\n\n\n\n1.3.3 Data analysis as the art of modelling\nNow that we have established an intuitive understanding of what a model is, we are ready for the most important insight you should take away from this chapter: Data analysis is the art of building and interpreting statistical models. What this means, and what we will see throughout this book, is that doing data analysis virtually always comes down to applying the fundamental principle described in the equation in Box1.1 (see Judd et al., 2017). Let us unpack what this equation means.\n\n\n\nThe fundamental principle of data analysis\n\n\\[\nData = Model + Error\n\\]\n\n\n\nFigure 1.3\n\n\n\nIn the context of a scientific study, the phenomenon we wish to explain will be represented by a set of data. Our task is to build, based on the information at our disposal, an explanatory model that best accounts for these data.\nSuppose you hypothesize that well-being depends on income. You ask 100 people their income (in dollars) and measure their well-being on a scale ranging from 1 to 10. The scores of well-being would be the data we are trying to explain, and the income is the information we can use to build a model. Applying our general equation in Box1.1 to this imaginary data, we could obtain something like this:\n\\[\nWellbeing_i=-5.261+0.005 \\times Income_i+Error_i\n\\] Or using more conventional notations:\n\\[\nY_i=-5.261+0.005X_i+e_i\n\\tag{1.1}\\]\nWhat the numbers in Equation 1.1 mean, and how to obtain them, is not important now. We will cover that in due course. What matters at this point is that you get a feel of how the elements making up the fundamental equation shown in Box1.1 might look like in a concrete case.\nNow, let us use this example to understand what our fundamental principle means. In Equation 1.1, the symbol \\(Y_i\\) represents Well-being, our dependent variable. This is the thing we are trying to explain, the Data. (The subscript \\(i\\) is there to remind us that \\(Y\\), Well-being, can take different values. Its exact meaning will become clear in the next chapters).\nThe Model is given by the expression \\(-5.261+0.005X_i\\), where \\(X_i\\) represents Income, our independent variable. A statistical model, like this one, is a mathematical expression, usually very simple, that generates guesses about what the data are. We say that the model makes predictions about the data. You can think of it as a little prediction machine. And we can give it information to base its predictions on. In this case, for instance, we used Income, \\(X_i\\), as an information for the model to use in generating predictions.\n\nA statistical model is a simple mathematical expression that generates guesses (predictions) about what each data point is.\nA model prediction is what a data point should be according to the model.\n\nBut remember that our model will, as a rule, be a simplification of reality; it will not completely explain the data. In practice, this means that there will be some discrepancies between our model predictions and the actual data. We say that our model will make errors. These errors are represented by the symbol \\(e_i\\) in Equation 1.1.\n\nAn error is the distance between a model prediction and the corresponding data point.\n\nMathematically, an error has a very straightforward definition. Given the general equation\n\\[\nData = Model + Error\n\\]\nwe can isolate the Error term by subtracting the Model term from sides of the equation:\n\\[\nData - Model = Error.\n\\]\nThus:\n\\[\nError = Data - Model\n\\]\nUsing conventional notations, we express exactly the same equation as\n\\[\ne_i = Y_i - \\hat{Y}_i\n\\]\nWhere \\(\\hat{Y}_i\\) represents the predictions made by our model.\nThus, it follows from our fundamental equation that the error corresponds to the difference between a given data point and the corresponding prediction made by the model.\n\n\n1.3.4 Two criteria that define a good model\nA statistical model, we saw, is just a mathematical expression that generates predictions about what the data are. But this definition is very broad: it leaves us with infinitely many options as to what particular model we choose for a given set of data. So, how do we choose? What model should we prefer over all others? There are two criteria that will guide us.\nThe first criterion is the most obvious: our model should make the least possible amount of errors in its predictions. At the end of the day, our purpose is to explain the phenomenon represented by the data. The more our model predictions differ from the data, the further it gets from achieving this purpose. In other words, the less errors our model makes, the better it explains the data.\nThe second criterion is more subtle. To understand it, remember why we use models in the first place. We know that the data we are trying to explain represent a complex reality generated by a vast amount of intricate causal processes. The point of using a model is to simplify this complexity by extracting relevant information that is useful to us (see Section 1.3.2). In other words, the very usefulness of a model lies in its simplicity. Thus, our model should be as simple as possible.\n\n\n\n\nTwo virtues of a good model\n\n\n\nAccuracy: A good model should make as little prediction error as possible.\n\n\nSimplicity: A good model should be as simple as possible.\n\n\n\n\nFigure 1.5\n\n\n\n\n\n\n\nDiener, E., Wirtz, D., Tov, W., Kim-Prieto, C., Choi, D., Oishi, S., & Biswas-Diener, R. (2010). New Well-being Measures: Short Scales to Assess Flourishing and Positive and Negative Feelings. Social Indicators Research, 97(2), 143–156. https://doi.org/10.1007/s11205-009-9493-y\n\n\nJudd, C. M., McClelland, G. H., & Ryan, C. S. (2017). Data analysis: A model comparison approach to regression, ANOVA, and beyond. Routledge.\n\n\nPopper, K. R. (2020). The Open Society and Its Enemies. Princeton University Press. (Original work published 1945)\n\n\nWeziak-Bialowolska, D., Bialowolski, P., Lee, M. T., Chen, Y., VanderWeele, T. J., & McNeely, E. (2021). Psychometric properties of flourishing scales from a comprehensive well-being assessment. Frontiers in Psychology, 12. https://doi.org/10.3389/fpsyg.2021.652209",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: turning questions into models</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction: turning questions into models",
    "section": "",
    "text": "The philosopher of science Karl Popper (Popper, 1945/2020) has criticized the view – which he associates with Aristotle – that concepts have a true essence that our definition should reflect. Rather, Popper argues, scientific concepts are just labels that we put on real-world processes. Thus, it does not really matter which label we use to refer to a given process, as long as we clearly define what our label refers to and use our terminology consistently. Popper refers to his view as “methodological nominalism” as opposed to Aristotle’s “methodological essentialism”.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: turning questions into models</span>"
    ]
  },
  {
    "objectID": "Logic.html",
    "href": "Logic.html",
    "title": "2  An introduction to inductive logic",
    "section": "",
    "text": "2.1 Arguments\nLet us assume that, as researchers, we strive to make statements about the world that are true. This probably sounds trivial (if it doesn’t, you might want to take a look at Digression 2.1). But it is important to acknowledge that distinguishing true statements from false ones is rarely, if ever, self-evident. It’s not something that we just know. If I tell you “capitalism is the only economic system that works”, you would be justified to ask “why should I believe that?”. You would expect me to provide you with reasons to believe that such a statement is true. Such reasons, together with the conclusion they are meant to support form what logicians call an argument.\nBecause they specify the reasons that we have for accepting statements as true, arguments should greatly matter to us. But an argument is not necessarily a good one. If you ask me “why should I believe that capitalism is the only economic system that works?” and my answer is “because Margaret said so”, you would be right to remain unconvinced. Even if she said so, Margaret could be mistaken. This a bad argument. Our goal now will be to understand better what makes a good argument. For that purpose, let us first look a what an argument is made of.\nArguments are made up of basic statements called propositions. We might say that propositions are the building blocks of arguments. An important feature of propositions is that they are either true or false. Thus, if I say “you are sitting on a chair”, I am uttering a proposition because my utterance is true or false depending on whether you are actually sitting on a chair. But if I say “are you sitting on a chair?” or “sit on a chair!”, I am not uttering a proposition, because questions and orders cannot be said to be true or false.\nTwo types of propositions form an argument: the premises and the conclusion. The premises are the propositions forming the basis on which the argument rests. They are the starting point of the argument. The conclusion, on the other hand, is the end point of the argument. It is the proposition which, according to the argument, is justified by the premises.\nIn everyday talk and writing, arguments rarely come with clear labels indicating which parts constitute their premises and which part constitutes their conclusion. But they always have at least one premise and a conclusion. So, they can always be re-stated to make their structure more transparent. For instance,\nWe can re-write the argument as follows:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to inductive logic</span>"
    ]
  },
  {
    "objectID": "Logic.html#arguments",
    "href": "Logic.html#arguments",
    "title": "2  An introduction to inductive logic",
    "section": "",
    "text": "An argument is a set of reasons presented in support of a conclusion.\n\n\n\n\n\n\nWhat do you mean “true”?\n\n\nIf the phrase “we should strive to make statements about the world that are true” sounds naïve to you, you might have picked up the habit – from your readings or classes – of thinking that there is no such thing as “objective truth”, that truth is ‘relative’, ‘socially constructed’, etc. If that’s the case, you are not alone. This kind of claim is surprisingly common within certain circles of social scientists (Lynch, 2004; Sidky, 2020). So, it is worth taking a moment to consider what we should make of them.\nLet’s first clarify in what sense I use the term “true”: A proposition is true only if it corresponds to a fact in reality (e.g., Chalmers, 1999; Rasmussen, 2014; Searle, 1995). For instance, the sentence “there is a box of chocolate in my fridge”, is a true proposition only if there indeed is a box of chocolate in my fridge. Thus, “truth” is a characteristic of certain propositions (for alternative definitions of truth, see Burgess & Burgess, 2011).\nNotice how, defined this way, truth is objective: it has nothing to do with what I think – or indeed with what anyone thinks. I might have forgotten that there is a box of chocolate in my fridge, and thus mistakenly believe that the proposition “there is a box of chocolate in my fridge” is false. But my belief is irrelevant, it does not change the fact that the proposition is true. If I were to open the fridge and see that there is a box in there, my belief about the truth of the proposition would change but the proposition would not ‘become’ true: it would have been true all along.\nIf all this sounds obvious to you, that’s good. Because claims about the ‘socially constructed’ or ‘relative’ nature of truth are usually grounded in a misunderstanding of this simple point. That is, such claims ultimately rest on a conflation of two kinds of statements: those of the form “x is true” and those of the form “y believes x to be true” or “x passes for true among y” (the philosopher Susan Haack (1999, Chapters 5, 8) calls this the “passes for” fallacy). A typical example can be found in the work of Michel Foucault, where this conflation is cultivated through ambiguous language. Consider for instance this often quoted passage (Foucault, 2001, p. 131):\n\nTruth is a thing of this world: it is produced only by virtue of multiple forms of constraint. And it induces regular effects of power. Each society has its regime of truth, its “general politics” of truth — that is, the types of discourse it accepts and makes function as true; the mechanisms and instances that enable one to distinguish true and false statements; the means by which each is sanctioned; the techniques and procedures accorded value in the acquisition of truth; the status of those who are charged with saying what counts as true.\n\nIf we keep in mind the difference between “truth” and “what passes for truth”, then the first sentence appears to make a radical claim: truth is “produced” (or, one might say, socially constructed). But when describing how this happens, the statement becomes ambiguous as to what exactly is produced. What does it mean that a society has “types of discourse it accepts and makes function as true”? Or that it has “mechanisms and instances that enable one to distinguish true and false statements”? Is the claim that (1) societies have institutions that influence what passes for true in that society? Or is it that (2) those societal institutions truly determine what is actually true in that society? If the claim is (1), then it is a convoluted way of saying something unremarkable. If the claim is (2), then it is obviously mistaken. The fact that, at the time of Galileo, the inquisition forbid any challenge to biblical geocentrism – the idea that the earth is immobile at the center of the universe – didn’t make geocentrism true. The earth didn’t wait for the permission of the inquisition to start orbiting around the sun. In sum, the claim that truth is somehow ‘constructed’ sounds plausible and profound only as long as “truth” and “what passes for truth” are used interchangeably. When the ambiguity is removed, the illusion of profundity dissolves. Try it yourself next time you encounter a similar claim.\nIn any case, this book assumes that empirical inquiry – and indeed any kind of inquiry – would be utterly futile without the notion of objective truth (e.g., Haack, 1999, Chapter 1). After all, what could we possibly learn from observing the world if nothing can be said to be objectively true?\n\n\n\nFigure 2.1\n\n\n\n\n\nPropositions are basic statements that are either true or false.\n\n\n\nThe premises are the propositions invoked to justify the conclusion.\nThe conclusion is the proposition inferred based on the premises.\n\n\n\nArgument (2)\nHow do we know that human cognition2 has been shaped by natural selection? Because it depends on how the brain is organized. And the organization of the brain, like that of any other organ in the human body, has been shaped by natural selection.\n\n\n\nArgument (2’)\nPremise 1: The organization of all organs of the human body has been shaped by natural selection.\nPremise 2: The human brain is an organ of the human body.\nPremise 3: Human cognition is shaped by the organization of the human brain.\nConclusion: Human cognition has been shaped by natural selection.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to inductive logic</span>"
    ]
  },
  {
    "objectID": "Logic.html#deductive-arguments",
    "href": "Logic.html#deductive-arguments",
    "title": "2  An introduction to inductive logic",
    "section": "2.2 Deductive arguments",
    "text": "2.2 Deductive arguments\nNot all arguments are of the same kind. Some can be trusted more than others. Deductive logic is concerned with identifying the kind of arguments that can be trusted the most, in the sense that they ensure a perfectly safe passage from premises to conclusion. Using the words of Ian Hacking (2001), we can say that deductive logic tries to identify “risk-free arguments”.\n\n2.2.1 Validity\nBut what does it mean for an argument to be “risk-free”? Consider Argument (2), for instance:\n\nArgument (3)\nPremise 1: The cat is either dead or alive.\nPremise 2: The cat is not dead.\nConclusion: The cat is alive.\n\nThe first premise tells us that one of two propositions – (1) the cat is dead; (2) the cat is alive – is necessarily true. Accepting that premise means accepting one of those two propositions. The second premise tells us that one proposition is not true. Thus, if we accept that second premise as well, we cannot escape the conclusion: it has to be true. If Premise 1 and 2 are both true, there is absolutely no way that the conclusion is not true. Logicians have a specific term to refer to this property: they say that the reasoning of such an argument is valid. For short, we can say: the argument is valid.\n\nA deductive argument is valid if and only if the truth of all the premises necessarily implies the truth of the conclusion. Otherwise, it is invalid.\n\nThe notion of validity is important because some arguments may pose as risk-free, while they actually are not. They may appear intuitively compelling to us but they are not, in fact, valid. We say that they are invalid. Consider the following:\n\nArgument (4)\nPremise 1: If Trillian is hungry, then she starts eating.\nPremise 2: Trillian starts eating.\nConclusion: Trillian is hungry.\n\nHere the conclusion – although it could happen to be true – does not necessarily follow from the premises. Premise 1 tells us that, if Trillian is hungry, we can be sure that she starts eating. But it does not tell us that if Trillian starts eating, we can be sure that she is hungry. Thus, the fact that Trillian starts eating (Premise 2) does not necessarily imply that she is hungry. Trillian might simply be bored or trying to please a host who invited her for dinner, for instance. Argument (4) commits a fallacy, i.e., an error in reasoning. This particular fallacy is called “affirming the consequent”.\nNote that whether an argument is valid or invalid depends on its form rather than its content. It doesn’t matter what the propositions that make up the argument refer to. To make that plain, we can re-write an argument by replacing propositions with arbitrary symbols, like letters. Argument (3), for instance, can be written as Argument (3’):\n\n\n\n\n\n\n\nArgument (3)\nPremise 1: The cat is either dead or alive.\nPremise 2: The cat is not dead.\nConclusion: The cat is alive.\n\n\n\n\nArgument (3’)\nPremise 1: p or q.\nPremise 2: not p.\nConclusion: q.\n\n\n\n\nYou can replace p and q by any pair of propositions you like, Argument (3’) will be valid3. And whenever you encounter an argument that can be re-written as Argument (2’), you know that this argument will be valid. This is why deductive logic is such a powerful tool: by identifying the forms of arguments that make them valid, it lets us distinguish valid arguments in any context.\n\n\n2.2.2 Soundness\nBut there is a very important caveat to keep in mind. We have seen that a valid argument ensures that the conclusion is true if the premises are all true. That is an important “if”. It means that even if an argument is valid, it can lead to a false conclusion. This can happen if it is based on at least one false premise. Look at Argument (4), for instance:\n\nArgument (5)\nPremise 1: All scientists love statistics\nPremise 2: All social scientists are scientists\nConclusion: All social scientists love statistics\n\nArgument (5) is perfectly valid but I have no doubt that the conclusion is false. This is because the first premise is false.\nThus, a valid argument does not guarantee that its conclusion will be true. It also needs to be based on premises that are all true. Only if both conditions are satisfied – i.e., the argument is valid and all its premises are true – can we be sure that the conclusion true. It that case, we say that the argument is sound (see Table 2.1).\n\nAn argument is sound if and only if its reasoning is valid and all its premises are true. Otherwise, it is unsound.\n\n\n\n\n\nTable 2.1: Determining the soundness of deductive arguments\n\n\n\nReasoning   ValidInvalidPremisesAll trueSoundUnsoundNot all trueUnsoundUnsound\n\n\n\n\n\n\n\n2.2.3 Why not only use deductive arguments?\nA valid deductive argument is the most trustworthy kind of argument: if its premises are true, it is sound, which means that we can be absolutely sure that its conclusion is true. So, if that’s the case, why not stick with that kind of argument? We could make sure to always start with true premises and then proceed to conclude through valid reasoning. This way, we would be sure to never end up with false conclusions. So why take the risk of relying on arguments that are not deductive?\nHere is a reason: deductive arguments don’t give us any guarantee that their premises are true, they just take them for granted. So, how do we know that the premises are true? You might think: well, we can make other deductive arguments that establish the truth of the premises. But these new arguments will need new premises of their own. How do we know if those are true? We could go on. But we would only be delaying the inevitable. We need to start somewhere eventually.\nHere is another way to see the problem. Deductive reasoning, by its very nature, only enables us to reach conclusions that are already contained in the premises. This means that, to arrive at a conclusion about facts in the real world, we would need premises that already tell us something about these real-world facts. Think about the kind of statements that we, researchers, are interested in. As social scientists in particular, we want to know if statements like “Income increases well-being”, “public education reproduces social inequalities” or “industrial societies foster an individualistic culture” are true. These are general statements about facts. And these can’t be derived from deductive reasoning without assuming other general factual statements that are logically related to your aimed conclusion. For instance,\n\nArgument (6)\nPremise (1): Anything that increases individuals’ ability to satisfy their needs also increases their well-being.\nPremise (2): Income increases people’s ability to satisfy their needs.\nConclusion: Income increases well-being.\n\nThis argument, though valid, has only displaced our problem: it makes our conclusion depend on two other general statements of facts, premise (1) and (2), which we don’t know if they are true (premise (1) is especially suspect). This illustrates how, to deduce general statements of facts, we always need to assume other general statements of facts. This is getting us nowhere.\nFortunately, there other kinds of arguments than deductive ones. An alternative that lets us avoid this problem consists in using statements about particular (as opposed to general) facts as premises. The key advantage is that we can make statements about particular facts based on what we directly observe. And, to the extent that statements about what we observe can be considered true4, this means that we would start from true premises. Such arguments based on (observed) particular facts are inductive arguments.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to inductive logic</span>"
    ]
  },
  {
    "objectID": "Logic.html#inductive-arguments",
    "href": "Logic.html#inductive-arguments",
    "title": "2  An introduction to inductive logic",
    "section": "2.3 Inductive arguments",
    "text": "2.3 Inductive arguments\nWe have seen that valid deductive arguments are “risk-free”: if their premises are true, their conclusion is necessarily true. Inductive arguments are not like that: even with true premises, the truth of the conclusion is never guaranteed. They are “risky” arguments (Hacking, 2001).\nDefining inductive arguments precisely is not an easy thing. We will worry about that later. For now, let us consider a couple of examples:\n\nArgument (7)\nBelgian people are not very polite. I can tell because my neighbour is Belgian and he is very rude.\n\n\nArgument (8)\nObservation of hundreds of societies of both humans and chimpanzees systematically support the view that the former are much more predisposed to cooperation than the latter.\n\nArgument (7) and (8) both use observations as evidence in support of a conclusion. Statements about observations are their premises. But there are many ways in which observations can be misleading. That makes these arguments risky. They are not valid arguments (in the sense defined in Section 2.2.1).\nBut if both arguments are risky, Argument (8) seems, in some sense, better than Argument (7). Can we say that Argument (8) is less risky? In practice, pretty much everyone would answer positively. And that is what we are going to assume moving forward. But you should be aware that this apparently simple question hides a very difficult philosophical problem known as the “problem of induction”. The most philosophically-minded will find a brief introduction to this problem in Digression 2.2. But, for our purpose, we can take for granted that some inductive arguments are less risky than others. Let us now examine why that is.\n\n\n\n\nThe problem of induction\n\n\nAs we saw in the introduction, most scientific knowledge necessarily relies on observations. How could we know how planets move, brains work, or humans behave, without observing them? But, because we can never observe every single planet, human, or brain at every single moment of their existence, any general conclusion we draw is based on our observations of a limited portion of reality. In short, we draw conclusions based on inductive arguments.\nIn the 18th century, the Scottish philosopher David Hume (1748/2007) posed what is arguably the toughest philosophical challenge science has ever had to face: he made the case that inductive arguments have no rational justification whatsoever. Any inductive argument, Hume says, is fundamentally flawed.\nTo understand Hume’s argument, imagine yourself in a room with him. He lights up a candle, points towards it, and the following dialogue ensues:\n\n\n[Hume] What is going to happen if you touch this flame?\n\n\n[You] Well, it’s going to hurt me.\n\n\n[Hume] How do you know?\n\n\n[You] I did it before and it hurt.\n\n\n[Hume] But how do you know that what happened last time with another flame will happen this time with this flame?\n\n\n[You] If it happened only once or twice that I was hurt immediately after touching a flame, it could be a coincidence. But I tried many times with many flames, always with the same result. Surely this means that touching any flame causes me to feel pain.\n\n\n[Hume] Are you certain? After all, you did not directly perceive any ‘causing’ (whatever this word means). The only thing you did perceive was a succession of two events: you touching a flame, then pain.\n\n\n[You] But if this succession has never failed to happen, no matter how many times I tried, surely I can conclude that reality works that way, that there is some sort of law of nature that connects these two events.\n\n\n[Hume] My friend, Bertrand, had a chicken that thought so. Every single day of the chicken’s life, it saw Bertrand coming and, then, the chicken got fed. “Here is a law of nature”, thought the chicken, as those two events never failed to be connected. Every single day, until the last one: Bertrand came and the chicken was cooked for dinner.5\n\n\nHume challenges us to explain what makes any inductive argument better than that of the chicken. His challenge can be summarized as follows: when we experience things (repeatedly) occurring together (e.g., touching a flame and feeling pain; the shape of a swan and the color white; twenty-four hours passing and the sun rising), we are tempted to believe that they will co-occur again in the future and/or have co-occurred similarly in the past. But no reasoning justifies that belief. Only an intuitive assumption that nature does not change its course. If we are asked to justify our belief that nature does not change its course, the only reason we can provide is: “because so far, in my experience, it hasn’t changed its course”. Thus, we end up using an inductive argument to justify inductive arguments. We are guilty of circular reasoning (a brilliant exposition of the problem can be found in Russell, 1912/2001, Chapter 6).\nThe most influential answer to Hume’s challenge came from the philosopher Karl Popper (1959, 1979). Popper’s solution is to avoid the problem entirely: Hume is right, Popper concedes, we cannot justify inductive arguments. We cannot infer the truth of general statements based on particular observations. But we don’t have to. Instead, we can show that some general statements are false by finding observations that contradict them. This is called falsification.\nTake the statement “All swans are white” (Popper, 1979). Trying to prove it true by observation would be futile: I could travel around the world and see as many white swans I like, I would have no guarantee that the next one would not be black. However, suffice it that I observe one single black swan to prove deductively that the initial statement is false:\n\nPremise 1: If all swans are white, then any particular swan is white.\nPremise 2: This particular swan is black.\nConclusion: Not all swans are white.\n\nBuilding on this idea, Popper’s view of empirical science’s progress is through the elimination (i.e., falsification) of false theories, rather than through the confirmation of true theories. This is a very smart move. Unfortunately, trying to use this falsification strategy to test real scientific hypotheses leads to important difficulties. For instance, deductive falsification is not applicable for hypotheses that have a statistical form (e.g., 90% of swans are white). Moreover, if simple statements like “all swans are white” have the merit of displaying the logic of falsification clearly, their simplicity conceals the fact that, in real scientific practice, both the theories themselves and the testing situations usually assume myriads of interrelated statements (e.g., theories that justify our measurement procedures). Thus, if an observation contradicts our prediction, it is not clear which statement exactly has been falsified.\nTo this day, Hume’s problem remains on open one that philosophers are actively trying to answer. And there currently are convincing proposals which, contrary to Popper’s, attempt to justify induction instead of avoiding it completely. Even sketching current discussions on this topic would be too ambitious, but the interested reader can find important contributions in Mayo (1996), Norton (2021), and Schurz (2019). Mayo’s (1996) approach, in particular, fits well with the view of data analysis that we will develop in later chapters.\n\n\n\nFigure 2.2\n\n\n\n\n2.3.1 Samples and populations\nThe most frequent kind of inductive argument takes the following form. You want to draw some factual conclusion about a set of “objects” (e.g., all humans, all people residing in a country, all potatoes). This set is your population. But accessing every object in your population is impossible in practice. Instead, you only have access to a subset of your population. This is called a sample. You then use observations made on your sample as a premise to justify your conclusion about the population.\n\nA population is a set of object about which we want to draw a factual conclusion.\nA sample is a subset of the population.\n\nImagine that you just taught a data analysis class to 100 students and you would like to know if they understood. You say “I made an exercise to see if you understood. If you can answer correctly, it means that you understood the class material”. There is a student that volunteers to do the exercise and she answers correctly. You might reason:\n\nArgument (9)\nPremise: This one student who volunteered understood the class material.\nConclusion: Most students in the class understood the class material.\n\nEven though you were careful to use “most students” instead of “all students” in your conclusion, that is a very risky argument. If this student in particular volunteered, it might be because she was confident that she understood. For all you know, all other students might have failed to understand. So, instead of relying on a volunteer, you might pick a student at random:\n\nArgument (10)\nPremise: This one student picked at random understood the class material.\nConclusion: Most students in the class understood the class material.\n\nThis is better. Since you picked a student randomly, there is no reason to expect her or him to be different from most other students. But the argument is still quite risky. Even if most students in the class did not understand, it could be that a single one picked at random happens to be one of those who did understand. That would not be so unlikely. Hence, it could still be that most students did not understand. So, instead of picking one student at random, you might decide to pick 10 of them:\n\nArgument (11)\nPremise: These ten students picked at random understood the class material.\nConclusion: Most students in the class understood the class material.\n\nThis is still risky, but less so because the conclusion is based on a larger sample, i.e., it is based on more data. One way to see this is to imagine that your conclusion is actually wrong: most students did not understand. If that were the case, would you expect that all ten students picked at random happen to be among the minority who understood? That sounds very unlikely.\nThis imaginary situation illustrates two general principles to judge how good inductive arguments involving a sample and a population are. These two principles are shown in Box 2.3.\n\n\n\n\nTwo virtues of inductive arguments\n\n\n\nRandom sampling: Inductive arguments based on a sample drawn at random from the population are less risky than those based on a non-random sample.\n\n\nSample size: Inductive arguments based on a comparatively large sample drawn are less risky than those based on a comparatively small sample.\n\n\n\n\nFigure 2.3\n\n\n\nIn the above examples, arguments all concluded about the population based on a sample. But this is only one among the three forms that an argument about samples and populations can take. Importantly, the principles stated in Box 2.3 apply to these three forms of inductive arguments.\n\n\n\n\nThree types of inductive arguments\n\n\nFrom sample to population: Premise: Statement about a sample. Conclusion: Statement about the population. From population to sample: Premise: Statement about the population. Conclusion: Statement about a sample. From sample to sample: Premise: Statement about a sample. Conclusion: Statement about another sample.\n\n\n\nFigure 2.4\n\n\n\n\n\n2.3.2 Probability\nIn everyday talk, when we utter risky arguments, we often signal this risk by attaching expressions like “probably” or “it is likely that” to our conclusion. We might thus reformulate the conclusion of Argument (11) as “Probably, most students understood the class material”. For practical purposes this is often enough. But can we be more specific and assign a precise value to the risk?\nThe notion of probability can help us do that. We all know intuitively how to use probability in the context of certain arguments that go from a population to a sample (see Box 2.4). Imagine, for instance, that I have a bag with 10 balls inside, 5 of which are black and 5 of which are white. If I pick a ball at random, what is the probability that I pick a black one? We all know intuitively understand that the probability is 50%, or 1/2.\nNow consider the same situation, and imagine that you pick a ball at random from the bag but don’t look at it. Then you say:\n\nArgument (12)\nPremise: There are 5 white balls and 5 black balls in the bag.\nConclusion: The ball that I picked at random is black.\n\nWhat value would you assign to the risk of this argument? You got it: 1/2. If the premise is true, you have a 50% probability of being wrong with that conclusion.\nWe will learn more about probabilities and how to compute them in the next chapter. But before that, we need the notion of a set.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to inductive logic</span>"
    ]
  },
  {
    "objectID": "Logic.html#reasoning-with-sets",
    "href": "Logic.html#reasoning-with-sets",
    "title": "2  An introduction to inductive logic",
    "section": "2.4 Reasoning with sets",
    "text": "2.4 Reasoning with sets\nThere is a branch of mathematics called Set theory that directly builds on logic. Unsurprisingly, it is based on the notion of set. We will define a set as follows:\n\nA set is a collection of objects considered as one object. The objects which are part of a set are called its members or its elements.\n\nSets can be given a rigorous mathematical definition. But we will leave that aside. What matters, for our purpose, is that the notion of set can make some arguments easier to evaluate. This is the case for arguments that use quantifiers like “all”, “some”, or “none”, because they can be represented through relations between sets. Most importantly, representation of events in terms of sets will prove useful to understand probabilities in the next chapter.\nAn example is shown in Figure 2.5, where Argument (5) is represented as the relation between sets in a diagram6. Through the diagram, we can see that, if we accept the first premise (placing the blue set completely inside the green set) and the second premise (placing the red set completely inside the blue set), we cannot escape the conclusion (the red set cannot be placed anywhere but fully within the green set).\n\n\n\n\n\n\n\nArgument (5)\nPremise 1: All scientists love statistics\nPremise 2: All social scientists are scientists\nConclusion: All social scientists love statistics\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Set representation of Argument (5)\n\n\n\n\n\n\n\nWhile visualizing relations between sets in diagrams is intuitive, drawing sets is not very practical (especially if you are considering many sets). It is thus useful to have symbols to express set relations. Symbols that will be important for us are shown in Table 2.2.\nLet us use a concrete example to make sense of those symbols. Among the set of all humans (\\(\\Omega\\)), some of them like chocolate (\\(C\\)) and some of them like Biscuits (\\(B\\)). These sets are represented with a Venn diagram in Figure 2.6. Using symbols, we can create expressions that designate new sets:\n\n\n\\(C \\cap B\\): Humans who like both chocolate and biscuits.\n\\(C \\cup B\\): Humans who like chocolate or biscuits or both.\n\\(C \\oplus B\\): Humans who either like chocolate or biscuits but not both.\n\\(C \\setminus B\\): Humans who like chocolate but not biscuits.\n\\(\\sim C\\): Humans who do not like chocolate.\n\n\nWe can also make propositions about sets. For instance:\n\n\n\\(B \\subset C\\): The set of humans who like biscuits is a subset of the set of humans who like chocolate. In other words, all humans who like biscuits also like chocolate.\n\\(B \\cap C = \\emptyset\\): The set of humans who like both chocolate and biscuits is empty. In other words, among humans, liking chocolate and liking biscuits are mutually exclusive.\n\n\n\n\n\n\nTable 2.2: Notations to express relations between sets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nName\nExample\nPronunciation\n\n\n\n\n\\(\\cap\\)\nIntersection\n\\(A \\cap B\\)\nA and B\n\n\n\\(\\cup\\)\nUnion\n\\(A \\cup B\\)\nA or B (inclusive)\n\n\n\\(\\oplus\\)\nExclusive or\n\\(A \\oplus B\\)\nA or B but not both (exclusive)\n\n\n\\(\\setminus\\)\nSet difference\n\\(A \\setminus B\\)\nA but not B\n\n\n\\(\\subset\\)\nProper subset\n\\(A \\subset B\\)\nA is a subset of B\n\n\n\\(\\sim\\)\nNegation\n\\(\\sim A\\)\nNot A\n\n\n\\(\\Omega\\)\nOmega\n\nUniversal set\n\n\n\\(\\emptyset\\)\nPhi\n\nEmpty set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: An example of sets\n\n\n\n\n\n\n\n\n\nBurgess, A. G., & Burgess, J. P. (2011). Truth (princeton foundations of contemporary philosophy). Princeton University Press.\n\n\nChalmers, A. (1999). What is this thing called science: An assessment of the nature and status of science and its methods (3rd ed.). Open University Press.\n\n\nFoucault, M. (2001). Essential works of foucault 1954-1984 volume 3: power (J. D. Faubion, R. Hurley, C. Gordon, & P. Rabinow, Eds.; 1st ed.). The New Press.\n\n\nHaack, S. (1999). Manifesto of a passionate moderate: Unfashionable essays. University of Chicago Press.\n\n\nHaack, S. (2003). Defending Science  within Reason: Between Scientism and Cynicism. Prometheus Books.\n\n\nHacking, I. (2001). An Introduction to Probability and Inductive Logic Desk Examination Edition. Cambridge University Press.\n\n\nHume, D. (2007). An enquiry concerning human understanding (oxford world’s classics) (First edition.). (Original work published 1748)\n\n\nLynch, M. P. (2004). True to life: Why truth matters. The MIT Press.\n\n\nMayo, D. G. (1996). Error and the growth of experimental knowledge (1st ed.). University Of Chicago Press. http://gen.lib.rus.ec/book/index.php?md5=d94ba8ec4b19d10955631d7a196beb33\n\n\nNorton, J. D. (2021). The material theory of induction. University of Calgary Press. https://library.oapen.org/handle/20.500.12657/57690\n\n\nPopper, K. R. (1959). The logic of scientific discovery. Routledge.\n\n\nPopper, K. R. (1979). Objective knowledge : an evolutionary approach. Oxford [Eng.] : Clarendon Press ; New York : Oxford University Press. http://archive.org/details/objectiveknowled00popp\n\n\nRasmussen, J. (2014). Defending the correspondence theory of truth. Cambridge University Press.\n\n\nRussell, B. (2001). The problems of philosophy (2nd ed.). Oxford University Press, USA. (Original work published 1912)\n\n\nSchurz, G. (2013). Philosophy of science: A unified approach. Routledge. http://gen.lib.rus.ec/book/index.php?md5=6c23fae40cb929f8863d3d9eb005c790\n\n\nSchurz, G. (2019). Hume’s Problem Solved: The Optimality of Meta-Induction. MIT Press.\n\n\nSearle, J. R. (1995). The construction of social reality. THE FREE PRESS.\n\n\nSidky, H. (2020). Science and anthropology in a post-truth world: A critique of unreason and academic nonsense. Lexington Books.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to inductive logic</span>"
    ]
  },
  {
    "objectID": "Logic.html#footnotes",
    "href": "Logic.html#footnotes",
    "title": "2  An introduction to inductive logic",
    "section": "",
    "text": "This chapter and the next very much relied upon Ian Hacking’s (2001) fantastic Introduction to Probability and Inductive Logic.↩︎\nThe term “cognition” refers to information processing happening in the brain. To use simple terms, we can say “thought processes”.↩︎\nAs long as p and q are not the same proposition.↩︎\nAlthough scientists typically take it for granted, whether the truth of observational statements is supported by our experience is debated among philosophers. This debate arises out of the fact that observational statements appear to be “theory-laden”: they always seem to imply assumptions that go beyond our direct experience. A solution to this problem lies in the idea that the meaning of words can be learned by “ostensive definition” (Haack, 2003; Schurz, 2013), which is a fancy way of saying that we learn the meaning of a word by being shown what it refers to.↩︎\nThe story of the chicken is adapted from Russell (1912/2001, p. 35).↩︎\nThis is called an Euler diagram.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to inductive logic</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Burgess, A. G., & Burgess, J. P. (2011). Truth (princeton\nfoundations of contemporary philosophy). Princeton University\nPress.\n\n\nChalmers, A. (1999). What is this thing called science: An\nassessment of the nature and status of science and its methods (3rd\ned.). Open University Press.\n\n\nDiener, E., Wirtz, D., Tov, W., Kim-Prieto, C., Choi, D., Oishi, S.,\n& Biswas-Diener, R. (2010). New Well-being Measures: Short Scales to\nAssess Flourishing and Positive and Negative Feelings. Social\nIndicators Research, 97(2), 143–156. https://doi.org/10.1007/s11205-009-9493-y\n\n\nFoucault, M. (2001). Essential works of foucault 1954-1984 volume 3:\npower (J. D. Faubion, R. Hurley, C. Gordon, & P. Rabinow, Eds.;\n1st ed.). The New Press.\n\n\nHaack, S. (1999). Manifesto of a passionate moderate: Unfashionable\nessays. University of Chicago Press.\n\n\nHaack, S. (2003). Defending Science  within Reason:\nBetween Scientism and Cynicism. Prometheus Books.\n\n\nHacking, I. (2001). An Introduction to Probability and Inductive\nLogic Desk Examination Edition. Cambridge University Press.\n\n\nHume, D. (2007). An enquiry concerning human understanding (oxford\nworld’s classics) (First edition.). (Original work published 1748)\n\n\nJudd, C. M., McClelland, G. H., & Ryan, C. S. (2017). Data\nanalysis: A model comparison approach to regression, ANOVA, and\nbeyond. Routledge.\n\n\nLynch, M. P. (2004). True to life: Why truth matters. The MIT\nPress.\n\n\nMayo, D. G. (1996). Error and the growth of experimental\nknowledge (1st ed.). University Of Chicago Press. http://gen.lib.rus.ec/book/index.php?md5=d94ba8ec4b19d10955631d7a196beb33\n\n\nNorton, J. D. (2021). The material theory of induction.\nUniversity of Calgary Press. https://library.oapen.org/handle/20.500.12657/57690\n\n\nPopper, K. R. (1959). The logic of scientific discovery.\nRoutledge.\n\n\nPopper, K. R. (1979). Objective knowledge : an evolutionary\napproach. Oxford [Eng.] : Clarendon Press ; New York : Oxford\nUniversity Press. http://archive.org/details/objectiveknowled00popp\n\n\nPopper, K. R. (2020). The Open Society and Its Enemies.\nPrinceton University Press. (Original work published 1945)\n\n\nRasmussen, J. (2014). Defending the correspondence theory of\ntruth. Cambridge University Press.\n\n\nRussell, B. (2001). The problems of philosophy (2nd ed.).\nOxford University Press, USA. (Original work published 1912)\n\n\nSchurz, G. (2013). Philosophy of science: A unified approach.\nRoutledge. http://gen.lib.rus.ec/book/index.php?md5=6c23fae40cb929f8863d3d9eb005c790\n\n\nSchurz, G. (2019). Hume’s Problem Solved: The Optimality of\nMeta-Induction. MIT Press.\n\n\nSearle, J. R. (1995). The construction of social reality. THE\nFREE PRESS.\n\n\nSidky, H. (2020). Science and anthropology in a post-truth world: A\ncritique of unreason and academic nonsense. Lexington Books.\n\n\nWeziak-Bialowolska, D., Bialowolski, P., Lee, M. T., Chen, Y.,\nVanderWeele, T. J., & McNeely, E. (2021). Psychometric properties of\nflourishing scales from a comprehensive well-being assessment.\nFrontiers in Psychology, 12. https://doi.org/10.3389/fpsyg.2021.652209",
    "crumbs": [
      "References"
    ]
  }
]